{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a353202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "good = TextBlob (\"good\").sentiment.polarity\n",
    "good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4ed5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6999999999999998"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = TextBlob(\"bad\").sentiment.polarity\n",
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e103582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.551115123125783e-17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_and_bad = TextBlob(\"good and bad\").sentiment.polarity\n",
    "good_and_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5ab644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(good+bad)/2 == good_and_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89afd079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_good = TextBlob(\"not_good\").sentiment.polarity\n",
    "not_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b2b59a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "very_good = TextBlob(\"very_good\").sentiment.polarity\n",
    "very_good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609b864",
   "metadata": {},
   "source": [
    "Listing 2:TextBlobexample for subjectivity calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0e19a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6000000000000001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "good = TextBlob (\"good\").sentiment.subjectivity\n",
    "good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414847f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awesome = TextBlob (\"awesome\").sentiment.subjectivity\n",
    "awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46f1f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awe_n_good = TextBlob (\"awesome and good\").sentiment.subjectivity\n",
    "awe_n_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ca8be85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(good+awesome)/2 == awe_n_good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13bbd7",
   "metadata": {},
   "source": [
    "Listing 3: Python Code######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f21ba48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- AAPL: 1m data not available for startTime=1606230000 and endTime=1606316400. The requested range must be within the last 30 days.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- AAPL: 5m data not available for startTime=1606230000 and endTime=1606316400. The requested range must be within the last 60 days.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- AAPL: 15m data not available for startTime=1606230000 and endTime=1606316400. The requested range must be within the last 60 days.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- AAPL: 15m data not available for startTime=1606230000 and endTime=1606316400. The requested range must be within the last 60 days.\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "######## Stock  Data  Stream ######################################\n",
    "import yfinance as yf\n",
    "\n",
    "ticker_list = [\"AAPL\"]\n",
    "path =\"./\"\n",
    "file_name =\"testtesttest\"\n",
    "start_date = \"2020-11-25\" \n",
    "last_date =\"2020-11-26\"\n",
    "for interval in [\"1m\", \"5m\" , \"15m\" , \"30m\", \"1h\"]:\n",
    "    stock_data = yf.download(ticker_list,start=start_date,end=last_date,interval = interval)\n",
    "    stock_data.to_csv(path+file_name + \"_\" + start_date + \"_\" + last_date  + \"_\"+ interval  + \".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f169fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 13:54:29.902305 tweepy\n",
      "2022-05-07 13:52:29.902340 tweepy\n",
      "2022-05-07 13:50:29.902470 tweepy\n",
      "2022-05-07 13:48:29.902344 tweepy\n",
      "2022-05-07 13:46:29.902492 tweepy\n",
      "2022-05-07 13:44:29.902399 tweepy\n",
      "@hojjat_na :))) اوهوم این tweepy خیلی چیز کاملیه. من توی apiش چیزی که اکانت‌های رایگان رو محدود کنه (به جز ریت لیمیت) ندیدم و گمونم هر کاری که خودمون می‌تونیم بکنیم رو اینم می‌تونه\n",
      "2022-05-07 13:42:29.902277 tweepy\n",
      "2022-05-07 13:40:29.902350 tweepy\n",
      "2022-05-07 13:38:29.902250 tweepy\n",
      "2022-05-07 13:36:29.902272 tweepy\n",
      "2022-05-07 13:34:29.902300 tweepy\n",
      "2022-05-07 13:32:29.902364 tweepy\n",
      "2022-05-07 13:30:29.902324 tweepy\n",
      "2022-05-07 13:28:29.902291 tweepy\n",
      "2022-05-07 13:26:29.902376 tweepy\n",
      "2022-05-07 13:24:29.902556 tweepy\n",
      "2022-05-07 13:22:29.902422 tweepy\n",
      "2022-05-07 13:20:29.902484 tweepy\n",
      "2022-05-07 13:18:29.902580 tweepy\n",
      "2022-05-07 13:16:29.902426 tweepy\n",
      "2022-05-07 13:14:29.902355 tweepy\n",
      "2022-05-07 13:12:29.902680 tweepy\n",
      "2022-05-07 13:10:29.902925 tweepy\n",
      "2022-05-07 13:08:29.902352 tweepy\n",
      "2022-05-07 13:06:29.905305 tweepy\n",
      "2022-05-07 13:04:29.902253 tweepy\n",
      "2022-05-07 13:02:29.902495 tweepy\n",
      "2022-05-07 13:00:29.902903 tweepy\n",
      "2022-05-07 12:58:29.907086 tweepy\n",
      "2022-05-07 12:56:29.902404 tweepy\n",
      "2022-05-07 12:54:29.902322 tweepy\n",
      "2022-05-07 12:52:29.902272 tweepy\n",
      "2022-05-07 12:50:29.902263 tweepy\n",
      "RT @iamukasa: Coolest two talks I've sat in today , an IBM package to find deviations in your dataset with fairness implications by @tanya_…\n",
      "2022-05-07 12:48:29.902318 tweepy\n",
      "RT @iamukasa: Coolest two talks I've sat in today , an IBM package to find deviations in your dataset with fairness implications by @tanya_…\n",
      "2022-05-07 12:46:29.902321 tweepy\n",
      "2022-05-07 12:44:29.902362 tweepy\n",
      "2022-05-07 12:42:29.902274 tweepy\n",
      "2022-05-07 12:40:29.902508 tweepy\n",
      "2022-05-07 12:38:29.902357 tweepy\n",
      "RT @iamukasa: Coolest two talks I've sat in today , an IBM package to find deviations in your dataset with fairness implications by @tanya_…\n",
      "Coolest two talks I've sat in today , an IBM package to find deviations in your dataset with fairness implications by @tanya_akumu  and a session on using both versions of the twitter API with a huge serving of Tweepy by  @VeldaKiara \n",
      "\n",
      "#PyConKE2022 \n",
      "#PyConKE https://t.co/vfR0BvIhgw\n",
      "2022-05-07 12:36:29.902326 tweepy\n",
      "2022-05-07 12:34:29.902425 tweepy\n",
      "2022-05-07 12:32:29.902274 tweepy\n",
      "2022-05-07 12:30:29.902306 tweepy\n",
      "2022-05-07 12:28:29.902334 tweepy\n",
      "2022-05-07 12:26:29.902366 tweepy\n",
      "2022-05-07 12:24:29.902291 tweepy\n",
      "2022-05-07 12:22:29.902262 tweepy\n",
      "2022-05-07 12:20:29.902267 tweepy\n",
      "2022-05-07 12:18:29.902264 tweepy\n",
      "2022-05-07 12:16:29.902287 tweepy\n",
      "2022-05-07 12:14:29.902360 tweepy\n",
      "2022-05-07 12:12:29.902543 tweepy\n",
      "2022-05-07 12:10:29.902359 tweepy\n",
      "2022-05-07 12:08:29.902284 tweepy\n",
      "2022-05-07 12:06:29.902453 tweepy\n",
      "2022-05-07 12:04:29.902468 tweepy\n",
      "2022-05-07 12:02:29.902257 tweepy\n",
      "2022-05-07 12:00:29.902250 tweepy\n",
      "2022-05-07 11:58:29.902278 tweepy\n",
      "2022-05-07 11:56:29.902278 tweepy\n",
      "2022-05-07 11:54:29.902378 tweepy\n",
      "2022-05-07 11:52:29.902322 tweepy\n",
      "2022-05-07 11:50:29.902252 tweepy\n",
      "2022-05-07 11:48:29.902323 tweepy\n",
      "2022-05-07 11:46:29.902582 tweepy\n",
      "2022-05-07 11:44:29.902417 tweepy\n",
      "2022-05-07 11:42:29.902487 tweepy\n",
      "2022-05-07 11:40:29.902468 tweepy\n",
      "2022-05-07 11:38:29.902297 tweepy\n",
      "2022-05-07 11:36:29.902279 tweepy\n",
      "2022-05-07 11:34:29.902620 tweepy\n",
      "2022-05-07 11:32:29.904171 tweepy\n",
      "2022-05-07 11:30:29.902321 tweepy\n",
      "2022-05-07 11:28:29.902290 tweepy\n",
      "2022-05-07 11:26:29.902321 tweepy\n",
      "2022-05-07 11:24:29.902486 tweepy\n",
      "2022-05-07 11:22:29.902255 tweepy\n",
      "2022-05-07 11:20:29.902261 tweepy\n",
      "2022-05-07 11:18:29.902263 tweepy\n",
      "2022-05-07 11:16:29.902284 tweepy\n",
      "2022-05-07 11:14:29.902412 tweepy\n",
      "2022-05-07 11:12:29.905285 tweepy\n",
      "2022-05-07 11:10:29.902216 tweepy\n",
      "2022-05-07 11:08:29.902286 tweepy\n",
      "2022-05-07 11:06:29.902251 tweepy\n",
      "2022-05-07 11:04:29.902313 tweepy\n",
      "Programming / Data Science tweet\n",
      "\n",
      "Looking to be a bit of #OSINT work related to Twitter and the analytics side. Any recommendations other than Tweepy or is that the way to go?\n",
      "2022-05-07 11:02:29.902413 tweepy\n",
      "2022-05-07 11:00:29.902379 tweepy\n",
      "2022-05-07 10:58:29.902250 tweepy\n",
      "2022-05-07 10:56:29.902268 tweepy\n",
      "2022-05-07 10:54:29.902263 tweepy\n",
      "2022-05-07 10:52:29.902355 tweepy\n",
      "2022-05-07 10:50:29.903602 tweepy\n",
      "2022-05-07 10:48:29.902475 tweepy\n",
      "2022-05-07 10:46:29.902488 tweepy\n",
      "2022-05-07 10:44:29.902350 tweepy\n",
      "2022-05-07 10:42:29.902410 tweepy\n",
      "2022-05-07 10:40:29.902271 tweepy\n",
      "2022-05-07 10:38:29.902281 tweepy\n",
      "2022-05-07 10:36:29.902283 tweepy\n",
      "2022-05-07 10:34:29.902264 tweepy\n",
      "2022-05-07 10:32:29.902247 tweepy\n",
      "2022-05-07 10:30:29.902278 tweepy\n",
      "2022-05-07 10:28:29.902266 tweepy\n",
      "2022-05-07 10:26:29.902267 tweepy\n",
      "Hello Tweepy 3\n",
      "2022-05-07 10:24:29.902271 tweepy\n",
      "Hello Tweepy 2\n",
      "2022-05-07 10:22:29.902537 tweepy\n",
      "2022-05-07 10:20:29.902470 tweepy\n",
      "2022-05-07 10:18:29.902272 tweepy\n",
      "2022-05-07 10:16:29.902279 tweepy\n",
      "2022-05-07 10:14:29.902408 tweepy\n",
      "2022-05-07 10:12:29.902397 tweepy\n",
      "2022-05-07 10:10:29.902472 tweepy\n",
      "2022-05-07 10:08:29.902518 tweepy\n",
      "2022-05-07 10:06:29.902325 tweepy\n",
      "2022-05-07 10:04:29.902565 tweepy\n",
      "RT @ErikDePay: Mooi lenteplaatje geeft deze roodborsttapuit.\n",
      "Mooie zaterdag Tweepy's. https://t.co/H6vfVu8Zsk\n",
      "Sending my first tweet via Tweepy!\n",
      "2022-05-07 10:02:29.902516 tweepy\n",
      "2022-05-07 10:00:29.902605 tweepy\n",
      "El tapir malayo #NationalGeographic #Fotodeldia #Python #Tweepy https://t.co/XXyPl9IU7j\n",
      "2022-05-07 09:58:29.902514 tweepy\n",
      "2022-05-07 09:56:29.902360 tweepy\n",
      "2022-05-07 09:54:29.902262 tweepy\n",
      "2022-05-07 09:52:29.902254 tweepy\n",
      "2022-05-07 09:50:29.902388 tweepy\n",
      "2022-05-07 09:48:29.902423 tweepy\n",
      "Mooi lenteplaatje geeft deze roodborsttapuit.\n",
      "Mooie zaterdag Tweepy's. https://t.co/H6vfVu8Zsk\n",
      "2022-05-07 09:46:29.902290 tweepy\n",
      "2022-05-07 09:44:29.902360 tweepy\n",
      "2022-05-07 09:42:29.902446 tweepy\n",
      "2022-05-07 09:40:29.902365 tweepy\n",
      "2022-05-07 09:38:29.902339 tweepy\n",
      "2022-05-07 09:36:29.902393 tweepy\n",
      "2022-05-07 09:34:29.902272 tweepy\n",
      "2022-05-07 09:32:29.902225 tweepy\n",
      "2022-05-07 09:30:29.902304 tweepy\n",
      "2022-05-07 09:28:29.902730 tweepy\n",
      "2022-05-07 09:26:29.904690 tweepy\n",
      "2022-05-07 09:24:29.902973 tweepy\n",
      "2022-05-07 09:22:29.906954 tweepy\n",
      "2022-05-07 09:20:29.903031 tweepy\n",
      "2022-05-07 09:18:29.903200 tweepy\n",
      "2022-05-07 09:16:29.903699 tweepy\n",
      "2022-05-07 09:14:29.903213 tweepy\n",
      "2022-05-07 09:12:29.902494 tweepy\n",
      "2022-05-07 09:10:29.903617 tweepy\n",
      "2022-05-07 09:08:29.906964 tweepy\n",
      "2022-05-07 09:06:29.909326 tweepy\n",
      "2022-05-07 09:04:29.907301 tweepy\n",
      "2022-05-07 09:02:29.906939 tweepy\n",
      "2022-05-07 09:00:29.919135 tweepy\n",
      "2022-05-07 08:58:29.910714 tweepy\n",
      "2022-05-07 08:56:29.910803 tweepy\n",
      "2022-05-07 08:54:29.903050 tweepy\n",
      "2022-05-07 08:52:29.903581 tweepy\n",
      "2022-05-07 08:50:29.902909 tweepy\n",
      "2022-05-07 08:48:29.904063 tweepy\n",
      "2022-05-07 08:46:29.902916 tweepy\n",
      "2022-05-07 08:44:29.906956 tweepy\n",
      "2022-05-07 08:42:29.903193 tweepy\n",
      "2022-05-07 08:40:29.903193 tweepy\n",
      "2022-05-07 08:38:29.906920 tweepy\n",
      "2022-05-07 08:36:29.903155 tweepy\n",
      "2022-05-07 08:34:29.903327 tweepy\n",
      "2022-05-07 08:32:29.904480 tweepy\n",
      "2022-05-07 08:30:29.911011 tweepy\n",
      "2022-05-07 08:28:29.903179 tweepy\n",
      "2022-05-07 08:26:29.904923 tweepy\n",
      "2022-05-07 08:24:29.902399 tweepy\n",
      "2022-05-07 08:22:29.903416 tweepy\n",
      "2022-05-07 08:20:29.902770 tweepy\n",
      "2022-05-07 08:18:29.903917 tweepy\n",
      "2022-05-07 08:16:29.902435 tweepy\n",
      "2022-05-07 08:14:29.907238 tweepy\n",
      "2022-05-07 08:12:29.902811 tweepy\n",
      "2022-05-07 08:10:29.906576 tweepy\n",
      "2022-05-07 08:08:29.903209 tweepy\n",
      "2022-05-07 08:06:29.905862 tweepy\n",
      "2022-05-07 08:04:29.903829 tweepy\n",
      "2022-05-07 08:02:29.903352 tweepy\n",
      "2022-05-07 08:00:29.906480 tweepy\n",
      "2022-05-07 07:58:29.903807 tweepy\n",
      "2022-05-07 07:56:29.904622 tweepy\n",
      "2022-05-07 07:54:29.904976 tweepy\n",
      "2022-05-07 07:52:29.905275 tweepy\n",
      "client = tweepy.Client(consumer_key=api_key,\n",
      "                       consumer_secret=api_secret,\n",
      "                       access_token=access_token,\n",
      "                       access_token_secret=token_secret)\n",
      "\n",
      "response = client.create_tweet(text='hello world')\n",
      "print(response)\n",
      "2022-05-07 07:50:29.905231 tweepy\n",
      "2022-05-07 07:48:29.902509 tweepy\n",
      "2022-05-07 07:46:29.902563 tweepy\n",
      "2022-05-07 07:44:29.903559 tweepy\n",
      "2022-05-07 07:42:29.903120 tweepy\n",
      "2022-05-07 07:40:29.904014 tweepy\n",
      "2022-05-07 07:38:29.903724 tweepy\n",
      "2022-05-07 07:36:29.902655 tweepy\n",
      "2022-05-07 07:34:29.905748 tweepy\n",
      "2022-05-07 07:32:29.902949 tweepy\n",
      "2022-05-07 07:30:29.911043 tweepy\n",
      "2022-05-07 07:28:29.907036 tweepy\n",
      "2022-05-07 07:26:29.904680 tweepy\n",
      "2022-05-07 07:24:29.902574 tweepy\n",
      "2022-05-07 07:22:29.923325 tweepy\n",
      "2022-05-07 07:20:29.905670 tweepy\n",
      "2022-05-07 07:18:29.902438 tweepy\n",
      "2022-05-07 07:16:29.909872 tweepy\n",
      "2022-05-07 07:14:29.903673 tweepy\n",
      "2022-05-07 07:12:29.902615 tweepy\n",
      "2022-05-07 07:10:29.902428 tweepy\n",
      "2022-05-07 07:08:29.902475 tweepy\n",
      "2022-05-07 07:06:29.902562 tweepy\n",
      "2022-05-07 07:04:29.907303 tweepy\n",
      "2022-05-07 07:02:29.906940 tweepy\n",
      "2022-05-07 07:00:29.902970 tweepy\n",
      "2022-05-07 06:58:29.903345 tweepy\n",
      "2022-05-07 06:56:29.904872 tweepy\n",
      "2022-05-07 06:54:29.903150 tweepy\n",
      "2022-05-07 06:52:29.910986 tweepy\n",
      "2022-05-07 06:50:29.903671 tweepy\n",
      "2022-05-07 06:48:29.902396 tweepy\n",
      "2022-05-07 06:46:29.904519 tweepy\n",
      "2022-05-07 06:44:29.906097 tweepy\n",
      "2022-05-07 06:42:29.909514 tweepy\n",
      "2022-05-07 06:40:29.907040 tweepy\n",
      "2022-05-07 06:38:29.915678 tweepy\n",
      "2022-05-07 06:36:29.903627 tweepy\n",
      "2022-05-07 06:34:29.906956 tweepy\n",
      "2022-05-07 06:32:29.910940 tweepy\n",
      "2022-05-07 06:30:29.913230 tweepy\n",
      "2022-05-07 06:28:29.909221 tweepy\n",
      "2022-05-07 06:26:29.904412 tweepy\n",
      "2022-05-07 06:24:29.912351 tweepy\n",
      "2022-05-07 06:22:29.917336 tweepy\n",
      "2022-05-07 06:20:29.912190 tweepy\n",
      "2022-05-07 06:18:29.907321 tweepy\n",
      "2022-05-07 06:16:29.904047 tweepy\n",
      "2022-05-07 06:14:29.906904 tweepy\n",
      "2022-05-07 06:12:29.903178 tweepy\n",
      "2022-05-07 06:10:29.906944 tweepy\n",
      "2022-05-07 06:08:29.904410 tweepy\n",
      "2022-05-07 06:06:29.940147 tweepy\n",
      "2022-05-07 06:04:29.902393 tweepy\n",
      "2022-05-07 06:02:29.903913 tweepy\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "client = tweepy.Client(\"AAAAAAAAAAAAAAAAAAAAANpRcQEAAAAAl335%2FjanD66CI6zD9IUU31t230g%3DOUEY4PmdUuW7SmoGViRWiKjy3bjzsJfkfWdWPXVkK71SUSLuk9\")\n",
    "\n",
    "for tweet in tweepy.Paginator(client.search_recent_tweets, \"Tweepy\",\n",
    "                                max_results=100).flatten(limit=250):\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eeaddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: extended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current search word: $AAPL\n",
      "2022-05-03\n",
      "sex <tweepy.cursor.ItemIterator object at 0x000001C1ED2775E0>\n",
      "<tweepy.cursor.ItemIterator object at 0x000001C1ED2775E0>\n"
     ]
    }
   ],
   "source": [
    "######## Twitter Data Stream######################################\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "#Plug in developer account keys\n",
    "consumer_key=\"AKBPcrkXF1pUYyRB6LFRc3vvm\"\n",
    "consumer_secret=\"wz1x6RtCAylj2RwOwRZiXMlNocSRIhiIyOVNDV20lqCad4LfSC\"\n",
    "access_token=\"1499819504355872768-oIA19QonOvSY1UxZPE7CisuTdg4Kea\"\n",
    "access_token_secret=\"Ix3BAIL7TrvYVOtJ8Rtkpm2AtWq4J9iC7Di1xC0wRIjb8\"\n",
    "\n",
    "#Give python your developer account keys\n",
    "auth = tw.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "#twitter api set ups\n",
    "api=tw.API(auth,\n",
    "          retry_count=10000,\n",
    "          retry_delay=60,\n",
    "          timeout=120)\n",
    "#Define variables below\n",
    "path=\"./\" #folder where the data will be saved\n",
    "start_date=\"2022-05-03\" #first day for extracting tweets\n",
    "last_date=\"2022-05-03\" #last day (included) for extracting tweets\n",
    "\n",
    "search_word_list=[\"$AAPL\",\"AAPL\"]\n",
    "\n",
    "######## Get Tweets######################################\n",
    "\n",
    "#search for each word in search list separately\n",
    "for search_word in search_word_list:\n",
    "    print(\"Current search word: \"+search_word)\n",
    "    \n",
    "    #search for each date separately\n",
    "    for date_since in pd.date_range(start=start_date,end=last_date):\n",
    "        df=pd.DataFrame([])\n",
    "        print(date_since.strftime('%Y-%m-%d'))\n",
    "        tweets=tw.Cursor(api.search_tweets,\n",
    "                         search_word,\n",
    "                         count = 100,\n",
    "                         extended=True\n",
    "                        ).items(250)\n",
    "                         #lang=\"en\",\n",
    "                         #until=(date_since+dt.timedelta(days=1)).strftime('%Y-%m-%d')).items()\n",
    "                         #since=date_since.strftime('%Y-%m-%d')).items()\n",
    "        print(\"sex\", tweets)\n",
    "        print(tweets)\n",
    "        #put content of object into list\n",
    "        for a in tweets:\n",
    "            print(\"sibal\")\n",
    "            print(a.user.screen_name)\n",
    "            break\n",
    "        print(\"oh!\")\n",
    "        users_locs=[[search_word,\n",
    "                    tweet.user.screen_name,\n",
    "                    tweet.user.location,\n",
    "                    tweet.text,\n",
    "                    tweet.created_at] for tweet in tweets]\n",
    "        \n",
    "        #make a df\n",
    "        tweet_text=pd.DataFrame(data=users_locs,\n",
    "                               columns=[\"search_word\",'user',\"location\",\"text\",\"date\"])\n",
    "        #merge the df with the other results for different dates and search words\n",
    "        df=df.append(tweet_text)\n",
    "        \n",
    "        #save daily results\n",
    "        df=df.reset_index(drop=True)\n",
    "        df.to_csv(path+search_word+\"_\"+date_since.strftime('%Y-%m-%d')+\".csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#\n",
    "# Functions\n",
    "#\n",
    "#############################################\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "\n",
    "# fct to concate daily twitter data to one df\n",
    "def concat_twitter_dat(path, search_word, start_date, last_date):\n",
    "    results = pd.DataFrame([])\n",
    "    for date in pd.date_range(start=start_date, end=last_date):\n",
    "        results = results.append(pd.read_csv(path +\n",
    "                                            search_word +\n",
    "                                            \"_\" +\n",
    "                                            date.strftime('%Y-%m-%d') + \".csv\", index_col=0))\n",
    "        return results.reset_index(drop=True)\n",
    "\n",
    "# fct to transform the date of the stock data to time zone\n",
    "def transform_stock_time(stock_data):\n",
    "    results = stock_data.copy()\n",
    "    results[\"date\"] = results.index.tz_convert('Europe/Berlin')\n",
    "    results[\"date\"] = results[\"date\"].dt.tz_localize(None)\n",
    "    results = results.reset_index(drop=True)\n",
    "    return results\n",
    "\n",
    "# Cleaning the tweets\n",
    "def cleanUpTweet(txt):\n",
    "    # Remove mentions (이상함)\n",
    "    txt = re.sub(r'@[A-Za-z0-9_]+', '', txt)\n",
    "    # Remove hashtags\n",
    "    txt = re.sub(r'#', '', txt)\n",
    "    # Remove retweets\n",
    "    txt = re.sub(r'RT : ', '', txt)\n",
    "    # Remove urls (이상함)\n",
    "    txt = re.sub(r'https?:|/|/[A-Za-z0-9\\.\\/]+', '', txt)\n",
    "    return txt\n",
    "\n",
    "# function to calculate subjectivity of tweets\n",
    "def getTextSubjectivity(txt):\n",
    "    return TextBlob(txt).sentiment.subjectivity\n",
    "\n",
    "# function to calculate polarity of tweets\n",
    "def getTextPolarity(txt):\n",
    "    return TextBlob(txt).sentiment.polarity\n",
    "\n",
    "# negative, natural, positive analysis\n",
    "def getTextAnalysis(a):\n",
    "    if a < 0:\n",
    "        return \"Negative\"\n",
    "    elif a == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "    \n",
    "# fct to clean tweets and to calculate polarity, subjectivity and tweet_class\n",
    "def prepare_twitter_variables(twitter_data):\n",
    "    results = twitter_data.copy()\n",
    "    results['clean_text'] = results['text'].apply(cleanUpTweet)\n",
    "    results[\"polarity\"] = results['clean_text'].apply(getTextPolarity)\n",
    "    results[\"subjectivity\"] = results['clean_text'].apply(getTextSubjectivity)\n",
    "    results[\"tweet_class\"] = results['polarity'].apply(getTextAnalysis)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# fct to merge stock and twitter data and create final variables\n",
    "def add_twitter_variables(stock_data,\n",
    "                         twitter_data,\n",
    "                         tweet_lag=5,\n",
    "                         subset_tweets_per=1):\n",
    "    #############################################\n",
    "    # Set ups\n",
    "    #############################################\n",
    "    # determine time interval of stock data (time diff (in seconds) / 60) = time diff in minutes\n",
    "    # stock_t_int = stock_time_interval\n",
    "    stock_t_int = int((stock_data[\"date\"][1] - stock_date[\"date\"][0]).seconds / 60)\n",
    "    # copy stock data df and create new columns\n",
    "    results = stock_data.copy()\n",
    "    results[\"tw_count\"] = \"NA\"     # number of tweets per stock_t_int\n",
    "    results[\"tw_mean\"] = \"NA\"      # mean of number of tweets per subset_twets_per in stock_t_int\n",
    "    results[\"tw_vola\"] = \"NA\"      # volatility of number of tweets per subset_tweets_per in stock_t_int\n",
    "    results[\"tw_min\"] = \"NA\"       # min number of tweets per subset_tweets_per in stock_t_int\n",
    "    results[\"tw_max\"] = \"NA\"       # max number of tweets per subset_tweets_per in stock_t_int\n",
    "    results[\"tw_chars\"] = \"NA\"     # avg number of characters of tweets per stock_t_int\n",
    "    results[\"tw_pola\"] = \"NA\"      # avg polarity of tweets\n",
    "    results[\"tw_subj\"] = \"NA\"      # avg subjectivity of tweets\n",
    "    results[\"tw_n_pos\"] = \"NA\"     # number of positive tweets\n",
    "    results[\"tw_n_neg\"] = \"NA\"     # number of negative tweets\n",
    "    results[\"tw_ratio_pos\"] = \"NA\" # share of positive tweets\n",
    "    results[\"tw_ratio_neg\"] = \"NA\" # share of negative tweets\n",
    "    \n",
    "    # loop through all rows\n",
    "    for i in range(0, stock_data.shape[0]):\n",
    "        # get current time, which is the time of the Open Price\n",
    "        # therefore, time interval of interest is Current Time + time interval of the data\n",
    "        # as we analyse the clos price\n",
    "        current_time = stock_data.iloc[i, :][\"date\"]\n",
    "        # calculate time range\n",
    "        cond_1 = twitter_data[\"date\"] < (current_time + dt.timedelta(minutes=stock_t_int - tweet_lag))\n",
    "        cond_2 = twitter_data[\"date\"] >= (current_time - dt.timedelta(minutes=tweet_lag))\n",
    "        #subset twitter data with tim range\n",
    "        twitter_subset = twitter_data.loc[cond_1 & cond_2, :].copy().reset_index(drop=True)\n",
    "        \n",
    "        #############################################\n",
    "        # Content unrelated variables\n",
    "        #############################################\n",
    "        # Compute Variable \"tw_count\"\n",
    "        results.loc[i, \"tw_count\"] = twitter_subset.shape[0]\n",
    "        \n",
    "        # set ups for variables calculated afterwards\n",
    "        tweets_per = pd.Series(np.zeros(stock_t_int))\n",
    "        for x in np.arange(subset_tweets_per, stock_t_int + 1, subset_tweets_per):\n",
    "            sub_cond_1 = twitter_subset[\"date\"] >= (current_time + dt.timedelta(minutes=(int(x) - 1 - tweet_lag)))\n",
    "            sub_cond_2 = twitter_subset[\"date\"] < (current_time + dt.timedelta(minutes=(int(x) - tweet.lag)))\n",
    "            tweets_per[x - 1] = twitter_subset.loc[sub_cond_1 & sub_cond_2, :].shape[0]\n",
    "            \n",
    "        # Compute Variables \"tw_mean\", \"tw_vola\", \"tw_min\", \"tw_max\"\n",
    "        results.loc[i, \"tw_mean\"] = tweets_per.mean()\n",
    "        results.loc[i, \"tw_vola\"] = tweets_per.var()\n",
    "        results.loc[i, \"tw_min\"] = tweets_per.min()\n",
    "        results.loc[i, \"tw_max\"] = tweets_per.max()\n",
    "        \n",
    "        # Compute Variable \"tw_chars\"\n",
    "        twitter_subset[\"tw_chars\"] = twitter_subset.text.apply(len)\n",
    "        results.loc[i, \"tw_chars\"] = twitter_subset[\"tw_chars\"].mean()\n",
    "        \n",
    "        #############################################\n",
    "        # Content related variables\n",
    "        #############################################\n",
    "        results.loc[i, \"tw_pola\"] = twitter_subset[\"polarity\"].mean()\n",
    "        results.loc[i, \"tw_subj\"] = twitter_subset[\"subjectivity\"].mean()\n",
    "        results.loc[i, \"tw_n_pos\"] = twitter_subset.loc[twitter_subset.tweet_class == \"Positive\", :]\n",
    "        results.loc[i, \"tw_n_neg\"] = twitter_subset.loc[twitter_subset.tweet_class == \"Negative\", :]\n",
    "        results.loc[i, \"tw_ratio_pos\"] = results.loc[i, \"tw_n_pos\"] / twitter_subset.shape[0]\n",
    "        results.loc[i, \"tw_ratio_neg\"] = results.loc[i, \"tw_n_neg\"] / twitter_subset.shape[0]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#\n",
    "#  Create  Model\n",
    "#\n",
    "#####################################################\n",
    "\n",
    "def build_model(hpt):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # number of hidden layer\n",
    "    hidden = hp.Int(\"n_hidden\",\n",
    "                   min_value = 0,\n",
    "                   max_value = 3)\n",
    "    \n",
    "    # first LSTM_Layer\n",
    "    model.add(LSTM(units = hp.Int(\"n_units1\",\n",
    "                                 min_value = 17,\n",
    "                                 max_value = 500,\n",
    "                                 step = 50),\n",
    "                  activation = hp.Choice(\"v_activation\",\n",
    "                                        values = [\"relu\", \"tanh\", \"sigmoid\"],\n",
    "                                        default = \"relu\"),\n",
    "                  input_shape = (X_train.shape[1], X_train.shape[2]),\n",
    "                  return_sequences = True if hidden > 0 else False))\n",
    "    \n",
    "    # add hidden layers (depends on before generated number for hidden)\n",
    "    if hidden > 0:\n",
    "        for lyaer in range(hidden):\n",
    "            model.add(Dropout(hp.Float(\"v_dropout_hidden\" + str(layer + 1),\n",
    "                                      min_value = 0.05,\n",
    "                                      max_value = 0.95,\n",
    "                                      step = 0.05)))\n",
    "            \n",
    "            model.add(LSTM(units = hp.Int(\"n_units_hidden\" + str(layer + 1).\n",
    "                                         min_value = 17,\n",
    "                                         max_value = 250,\n",
    "                                         step = 50),\n",
    "                          activation = 'relu',\n",
    "                          return_sequences = True if layer != hidden else False))\n",
    "            \n",
    "        model.add(Dropout(hp.Float(\"v_dropout\",\n",
    "                                  min_value = 0.05,\n",
    "                                  max_value = 0.95,\n",
    "                                  step = 0.05)))\n",
    "        model.add(Dense(units = 1))\n",
    "        \n",
    "        model.compile(optimizer = \"adam\",\n",
    "                     loss = 'mean_squared_error')\n",
    "        \n",
    "        return model\n",
    "\n",
    "#####################################################\n",
    "#\n",
    "#  Search  For  Best  Model\n",
    "#\n",
    "#####################################################\n",
    "tuner = RandomSearch(build_model,\n",
    "                    objective = 'val_lostt',\n",
    "                    max_trials = 50,\n",
    "                    seed = 1,\n",
    "                    executions_per_trial = 3,\n",
    "                    directory = \"\")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10,\n",
    "                                                 restore_best_weights = True)\n",
    "tuner.search(X_train,\n",
    "            y_train,\n",
    "            epochs = 1500,\n",
    "            batch_szie = 32,\n",
    "            validation_split = 0.2,\n",
    "            callbacks = [early_stopping_cb],\n",
    "            verbose = 1)\n",
    "# Retrieve the best model,\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# save best model for that variable combination\n",
    "best_model.save(\"best_model.h5\")\n",
    "\n",
    "# Evaluate the best model with test data\n",
    "loss = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
