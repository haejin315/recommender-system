{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df27a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "498bcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert:\n",
    "    def __init__(self):\n",
    "        model = torch.load('./bert_model')\n",
    "    \n",
    "    def pred_tweet(self, tweet_data):\n",
    "        document_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in tweet_data]\n",
    "        tokenized_texts = [tokenizer.tokenize(s) for s in document_bert]\n",
    "        MAX_LEN = 512\n",
    "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            seq_mask = [float(i>0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        train_inputs = torch.tensor(input_ids)\n",
    "        train_labels = torch.zeros(len(input_ids)) # 라벨이 없는 트위터 데이터므로 0을 채워넣음, 트위터 개수\n",
    "        train_masks = torch.tensor(attention_masks)\n",
    "        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "        model.eval()\n",
    "        # 변수 초기화\n",
    "        pred = []\n",
    "        # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # 경과 정보 표시\n",
    "            if step % 100 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # 배치를 GPU에 넣음\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # 배치에서 데이터 추출\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # 그래디언트 계산 안함\n",
    "            with torch.no_grad():     \n",
    "                # Forward 수행\n",
    "                outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            # 로스 구함\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # CPU로 데이터 이동\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            #예측된 값을 저장\n",
    "            pred += np.argmax(logits, axis=1).flatten().tolist()\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fb4a421",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './bert_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-bdc41903cfd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-682dfdda48db>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mbert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./bert_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpred_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './bert_model'"
     ]
    }
   ],
   "source": [
    "a = bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "511aa9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [['i love you']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cd2dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haeji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haeji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\haeji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: [Errno 13] Permission denied: './data\\\\.ipynb_checkpoints'\n",
      "error: [Errno 13] Permission denied: './data\\\\aclImdb'\n",
      "       search_word                     user  \\\n",
      "0         GameStop              Marcus Penn   \n",
      "1         GameStop  h0bbes. Yeah, that one.   \n",
      "2         GameStop                Omer Zach   \n",
      "3         GameStop             Bill English   \n",
      "4         GameStop                     Mae.   \n",
      "...            ...                      ...   \n",
      "102531       Tesla            Martin Burris   \n",
      "102532       Tesla           UNICORN FOR PM   \n",
      "102533       Tesla   Blocked by Sawyer Club   \n",
      "102534       Tesla                  News 10   \n",
      "102535       Tesla             Dan Burkland   \n",
      "\n",
      "                                                    tweet  \\\n",
      "0       Yay geeking out! (@ GameStop) http://4sq.com/a...   \n",
      "1       I'm at GameStop (8336 Agora Pkwy, The Forum, S...   \n",
      "2       I just ousted \\n@braisinhussy\\n as the mayor o...   \n",
      "3       I'm at Gamestop in Trenton, NJ http://gowal.la...   \n",
      "4       I'm at Gamestop East Brunswick. http://4sq.com...   \n",
      "...                                                   ...   \n",
      "102531  To anyone who can read this tweet and is a gun...   \n",
      "102532  Tesla is on . Literally. Again!\\nJulia Bagg\\n@...   \n",
      "102533  NEWS: Tesla has filed a rule change for grid o...   \n",
      "102534  \"Luckily, it didn't happen while we were drivi...   \n",
      "102535  For those paying attention, this also confirms...   \n",
      "\n",
      "                            date comments likes retweets  \n",
      "0       2010-05-01T17:40:33.000Z      NaN   NaN      NaN  \n",
      "1       2010-05-01T00:41:14.000Z      NaN   NaN      NaN  \n",
      "2       2010-05-02T00:14:45.000Z      NaN   NaN      NaN  \n",
      "3       2010-05-04T15:50:35.000Z      NaN   NaN      NaN  \n",
      "4       2010-05-04T19:55:32.000Z      NaN   NaN      NaN  \n",
      "...                          ...      ...   ...      ...  \n",
      "102531  2022-05-24T00:00:07.000Z      NaN   NaN      NaN  \n",
      "102532  2022-05-24T00:02:01.000Z      NaN   NaN      NaN  \n",
      "102533  2022-05-24T00:01:22.000Z      NaN   NaN      NaN  \n",
      "102534  2022-05-24T00:00:00.000Z      NaN   NaN      NaN  \n",
      "102535  2022-05-24T00:04:01.000Z      NaN   NaN      NaN  \n",
      "\n",
      "[102536 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "## 1. 트윗 전처리\n",
    "\n",
    "file_dir = './data'\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# 디렉토리에서 트윗 파일 불러오기\n",
    "for file_name in os.listdir(file_dir):\n",
    "    # 트윗 파일이 아니면 넘김\n",
    "    #if ('PLTR' not in file_name) and ('palantir' not in file_name):\n",
    "    if 'stock' in file_name:\n",
    "        print(f\"FIle skipped (not tweet): {file_name}\")\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(file_dir, file_name)\n",
    "    \n",
    "    # 트윗 파일 로드 및 병합\n",
    "    try:\n",
    "        file = pd.read_csv(file_path, index_col=0)\n",
    "        if len(file) == 0:\n",
    "            continue\n",
    "        df = df.append(file)\n",
    "\n",
    "    except Exception as error_message:\n",
    "        print(f'error: {error_message}')\n",
    "        \n",
    "# 인덱스 초기화\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.rename(columns={'text':'tweet'})\n",
    "print(df)\n",
    "def clean_tweet(tweet):\n",
    "    tweet = str(tweet)\n",
    "    tweet = tweet.lower() # 소문자로 바꾸기\n",
    "    tweet = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', tweet) # URL 제거\n",
    "    tweet = re.sub('\\$[a-zA-Z0-9]*', ' ', tweet) # ticker symbol($로 시작하는 주식 관련 심볼) 제거\n",
    "    tweet = re.sub('\\@[a-zA-Z0-9]*', ' ', tweet) # 유저 호출하는 기능(@로 시작) 제거\n",
    "    tweet = re.sub('[^a-zA-Z\\']', ' ', tweet) # 문자가 아닌 것 제거\n",
    "    tweet = ' '.join( [w for w in tweet.split() if len(w)>1] )\n",
    "    \n",
    "    tweet = ' '.join([lemma.lemmatize(x) for x in nltk.wordpunct_tokenize(tweet) if x not in stop_words])\n",
    "    tweet = [lemma.lemmatize(x, nltk.corpus.reader.wordnet.VERB) for x in nltk.wordpunct_tokenize(tweet) if x not in stop_words]\n",
    "    return tweet \n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# 트윗을 토큰화시킨 것, 그리고 토큰을 이어붙인 것을 새로운 열에 추가\n",
    "df[\"clean_tweet\"] = df[\"tweet\"].apply(lambda x:clean_tweet(x))\n",
    "df[\"cleaned_tweet\"] = df[\"clean_tweet\"].apply(lambda x:' '.join(x))\n",
    "df = df[df['search_word'] == 'Tesla'] # 주식 이름 선택\n",
    "tweet_data = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80b4b7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8004                       Dads new car tesla model x\\n1\\n2\n",
       "8005      What an amazing machine! resolution to GoGreen...\n",
       "8006      Tesla appears to be suffering from to much #Ne...\n",
       "8007      Shahed: what if tesla had a New Year sale and ...\n",
       "8008      Tesla Model S Burns To A Crisp During Supercha...\n",
       "                                ...                        \n",
       "102531    To anyone who can read this tweet and is a gun...\n",
       "102532    Tesla is on . Literally. Again!\\nJulia Bagg\\n@...\n",
       "102533    NEWS: Tesla has filed a rule change for grid o...\n",
       "102534    \"Luckily, it didn't happen while we were drivi...\n",
       "102535    For those paying attention, this also confirms...\n",
       "Name: tweet, Length: 94532, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a18900ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pred_tweet() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-28952916526b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: pred_tweet() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "a.pred_tweet(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d8418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
